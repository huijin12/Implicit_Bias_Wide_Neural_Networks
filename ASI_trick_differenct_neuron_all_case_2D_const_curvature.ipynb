{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from networks import *\n",
    "from normal_train import *\n",
    "import numpy as np\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I list some settings, you can swap to other settings by commenting and uncommenting\n",
    "\n",
    "args = dict()\n",
    "\n",
    "# training inputs\n",
    "args['xp'] = np.array([[-1,-1], [1,1], [0,0], [-1,1], [1,-1]],dtype=np.float32)\n",
    "# other training inputs\n",
    "# args['xp'] = np.array([[-1.3,-0.7], [0.5,0.9], [-0.8,0.3], [-0.4,1.6], [1.6,-0.4]],dtype=np.float32)\n",
    "\n",
    "# training outputs\n",
    "args['fp'] = np.array([[1.5], [1.5], [0.5], [-0.5], [-0.5]],dtype=np.float32)\n",
    "# other training outputs\n",
    "# args['fp'] = np.array([1.5, 1.5, 0.5, -0.5, -0.5],dtype=np.float32)\n",
    "# args['fp'] = np.array([[0.05], [0.02], [0], [-0.01], [-0.03]],dtype=np.float32)\n",
    "\n",
    "#number of inputs\n",
    "args['input_dim'] = 2\n",
    "#name of the saved model\n",
    "args['tag'] = \"2d_toy\"\n",
    "#whether to only train the output layer\n",
    "args['train_only_output'] = True\n",
    "# args['bias'] = (\"uniform\", 2)\n",
    "args['bias'] = (\"uniform\", 2)\n",
    "args['weight'] = \"unit_vector\"\n",
    "args['path'] = '/Users/jinhui/Documents/GitHub/model'\n",
    "args['epochs'] = 10000\n",
    "# args['nums_of_neurons']=[10,20,40,80,160,320,640,1280,2560]\n",
    "args['nums_of_neurons']=[10]\n",
    "\n",
    "# Gaussian initialzation\n",
    "\n",
    "# args = dict()\n",
    "# args['xp'] = np.array([[-1,-1], [1,1], [0,0], [-1,1], [1,-1]],dtype=np.float32)\n",
    "# args['fp'] = np.array([[1.5], [1.5], [0.5], [-0.5], [-0.5]],dtype=np.float32)\n",
    "# args['input_dim'] = 2\n",
    "# args['tag'] = \"2d_toy_gaussian_try\"\n",
    "# args['train_only_output'] = True\n",
    "# # args['bias'] = (\"uniform\", 2)\n",
    "# sigmab = 0.1\n",
    "# args['bias'] = (\"normal\", sigmab)\n",
    "# args['weight'] = \"normal\"\n",
    "# args['path'] = '/Users/jinhui/Documents/GitHub/model'\n",
    "# args['epochs'] = 10000\n",
    "# args['nums_of_neurons']=[10,20,40,80,160,320,640,1280,2560]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute f\n"
     ]
    }
   ],
   "source": [
    "# this block compute the solution of the variational problem for uniform initialzation \n",
    "# i.e. the weight uniformly distribute on the sphere and the bias uniformly distribute on a interval\n",
    "\n",
    "# solution of the variational problem with linear correction ((eq.10) in paper)\n",
    "# valid only under the condition of Theorem 7\n",
    "# if weights and biases are initialized by Gaussian distribution, use the next cell\n",
    "\n",
    "numgrid = 21 # you can choose larger numgrid to make the grid dense\n",
    "R = np.abs(args['xp']).max()\n",
    "x = np.linspace(-R, R, numgrid, dtype=np.float32)\n",
    "h = x[1]-x[0]\n",
    "y = np.linspace(-R, R, numgrid, dtype=np.float32)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "X = np.expand_dims(X, axis=2)\n",
    "Y = np.expand_dims(Y, axis=2)\n",
    "\n",
    "M = args['xp'].shape[0]\n",
    "d = args['xp'].shape[1]\n",
    "coeff = np.zeros((M+d+1,M+d+1))\n",
    "for i in range(M):\n",
    "    coeff[i,0:M] = np.linalg.norm(args['xp']-args['xp'][i], axis=1)**3\n",
    "    coeff[i,M:M+d] = args['xp'][i]\n",
    "    coeff[i,M+d] = 1\n",
    "    coeff[M:M+d, i] = args['xp'][i]\n",
    "    coeff[M+d, i] = 1\n",
    "\n",
    "lambda_u_v = np.linalg.solve(coeff, np.concatenate((args['fp'].squeeze(),np.zeros(3))))\n",
    "\n",
    "u_first = lambda_u_v[-3:-1]\n",
    "v_first = lambda_u_v[-1]\n",
    "\n",
    "fx = x.copy()\n",
    "fy = y.copy()\n",
    "f_variation_exact = np.zeros((fx.shape[0], fy.shape[0]))\n",
    "fX, fY = np.meshgrid(fx, fy)\n",
    "xi = np.zeros((2))\n",
    "\n",
    "print(\"compute f\")\n",
    "for idx, ix in enumerate(fx):\n",
    "    for idy, iy in enumerate(fy):\n",
    "        xi[0] = ix\n",
    "        xi[1] = iy\n",
    "        f_variation_exact[idy,idx]=np.dot(lambda_u_v, \n",
    "                                          np.concatenate((np.linalg.norm(args['xp']-xi, axis=1)**3,xi,np.ones(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this block compute the solution of the optimization problem for gaussian initialzation \n",
    "# # if you use Gauusian initialzation, uncomment the whole block\n",
    "# # follow the instrcution in the comments to use the following code\n",
    "\n",
    "# # solution of the optimization problem with linear correction ((eq.24) in paper)\n",
    "\n",
    "# def continuous_version_with_linear(args):\n",
    "#     linear_constraint_weight = np.zeros((args[\"xp\"].shape[0], Angle.shape[0]+3))\n",
    "#     for idx, train_sample in enumerate(args[\"xp\"]):\n",
    "#         linear_constraint_weight[idx,0:-3] = np.maximum(np.cos(Angle)*train_sample[0]+np.sin(Angle)*train_sample[1]+Bias,0)*weight.reshape(-1)\n",
    "#         linear_constraint_weight[idx,-3] = train_sample[0]\n",
    "#         linear_constraint_weight[idx,-2] = train_sample[1]\n",
    "#         linear_constraint_weight[idx,-1] = 1\n",
    "\n",
    "#     linear_constraint_with_linear = LinearConstraint(linear_constraint_weight, args[\"fp\"].reshape(-1), args[\"fp\"].reshape(-1))\n",
    "\n",
    "#     from scipy.optimize import minimize\n",
    "#     def obj_with_linear(x):\n",
    "#         return np.sum(x[0:-3]**2*weight.reshape(-1))\n",
    "\n",
    "#     def obj_der_with_linear(x):\n",
    "#         return np.concatenate((2*x[0:-3],np.array([0,0,0])))\n",
    "\n",
    "\n",
    "#     def obj_hess_p_with_linear(x, p):\n",
    "#         return np.concatenate((2*p[0:-3],np.array([0,0,0])))\n",
    "\n",
    "#     x0 = np.zeros(angle.shape[0]*bias.shape[0]+3)\n",
    "\n",
    "\n",
    "#     return minimize(obj_with_linear, x0, method='trust-constr', jac=obj_der_with_linear, hessp=obj_hess_p_with_linear,\n",
    "#                 constraints=[linear_constraint_with_linear],\n",
    "#                 options={'verbose': 1, 'xtol':1e-20, 'gtol':1e-10})\n",
    "\n",
    "\n",
    "# # solution of the optimization problem without linear correction ((eq.19) in paper)\n",
    "\n",
    "# def continuous_version(args):\n",
    "\n",
    "#     linear_constraint_weight = np.zeros((args[\"xp\"].shape[0], Angle.shape[0]))\n",
    "#     for idx, train_sample in enumerate(args[\"xp\"]):\n",
    "#         linear_constraint_weight[idx] = np.maximum(np.cos(Angle)*train_sample[0]+np.sin(Angle)*train_sample[1]+Bias,0)*weight.reshape(-1)\n",
    "\n",
    "\n",
    "#     linear_constraint = LinearConstraint(linear_constraint_weight, args[\"fp\"].reshape(-1), args[\"fp\"].reshape(-1))\n",
    "\n",
    "#     from scipy.optimize import minimize\n",
    "#     def obj(x):\n",
    "#         return np.sum(x**2*weight.reshape(-1))\n",
    "\n",
    "#     def obj_der(x):\n",
    "#         return 2*x\n",
    "\n",
    "\n",
    "#     def obj_hess_p(x, p):\n",
    "#         return 2*p\n",
    "\n",
    "#     x0 = np.zeros(angle.shape[0]*bias.shape[0])\n",
    "\n",
    "\n",
    "#     return minimize(obj, x0, method='trust-constr', jac=obj_der, hessp=obj_hess_p,\n",
    "#                     constraints=[linear_constraint],\n",
    "#                     options={'verbose': 1})\n",
    "\n",
    "# num_angle = 601\n",
    "# angle = np.linspace(0, 2*math.pi, num_angle, dtype=np.float32)\n",
    "# angle = angle[0:-1]\n",
    "# num_bias = 601\n",
    "# bias = np.linspace(-2, 2, num_bias, dtype=np.float32) \n",
    "\n",
    "# sigmab = 1 # standard deviation of initialization of bias, change it if you use different standard deviation\n",
    "# sigmaw = 1 # standard deviation of initialization of weight, change it if you use different standard deviation\n",
    "\n",
    "# weight = np.ones((angle.shape[0], bias.shape[0]))\n",
    "# weight[0] = 1/2\n",
    "# weight[-1] = 1/2\n",
    "# weight[:,0] = 1/2\n",
    "# weight[:,-1] = 1/2\n",
    "# weight = weight/(sigmab*sigmab+bias*bias*sigmaw*sigmaw)**2.5\n",
    "# weight = weight.T\n",
    "\n",
    "# Angle, Bias = np.meshgrid(angle, bias)\n",
    "# Angle = Angle.reshape(-1)\n",
    "# Bias = Bias.reshape(-1)\n",
    "\n",
    "# # change to the code in the comment if you compute the solution of optimization problem with linear correction\n",
    "# # res_with_linear = continuous_version_with_linear(args)\n",
    "# res_non_linear = continuous_version(args)\n",
    "\n",
    "# fx = x.copy()\n",
    "# fy = y.copy()\n",
    "\n",
    "# # change to the code in the comment if you compute the solution of optimization problem with linear correction\n",
    "# # f_variation_linear_adjust = np.zeros((fx.shape[0], fy.shape[0]))\n",
    "# f_variation_no_linear_adjust = np.zeros((fx.shape[0], fy.shape[0]))\n",
    "\n",
    "# fX, fY = np.meshgrid(fx, fy)\n",
    "# xi = np.zeros((2))\n",
    "\n",
    "# print(\"compute f\")\n",
    "\n",
    "# # change to the code in the comment if you compute the solution of optimization problem with linear correction\n",
    "# # for idx, ix in enumerate(fx):\n",
    "# #     for idy, iy in enumerate(fy):\n",
    "# #         xi[0] = ix\n",
    "# #         xi[1] = iy\n",
    "# #         first_layer = np.maximum(np.cos(Angle)*ix+np.sin(Angle)*iy+Bias,0)*weight.reshape(-1)\n",
    "# #         f_variation_linear_adjust[idy,idx]=np.sum(first_layer*res_with_linear.x[0:-3])+\\\n",
    "# #             res_with_linear.x[-3]*ix+res_with_linear.x[-2]*iy+res_with_linear.x[-1]\n",
    "\n",
    "# for idx, ix in enumerate(fx):\n",
    "#     for idy, iy in enumerate(fy):\n",
    "#         xi[0] = ix\n",
    "#         xi[1] = iy\n",
    "#         first_layer = np.maximum(np.cos(Angle)*ix+np.sin(Angle)*iy+Bias,0)*weight.reshape(-1)\n",
    "#         f_variation_no_linear_adjust[idy,idx]=np.sum(first_layer*res_non_linear.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from networks import *\n",
    "from normal_train import *\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "def sum_of_all_parameter(model):\n",
    "    res = 0\n",
    "    if isinstance(model, TwoLayerReluASI):\n",
    "        for p in model.features1:\n",
    "            if p.__class__.__name__==\"Linear\":\n",
    "                res+=np.linalg.norm(p.weight.detach().numpy().squeeze())\n",
    "\n",
    "        for p in model.features2:\n",
    "            if p.__class__.__name__==\"Linear\":\n",
    "                res+=np.linalg.norm(p.weight.detach().numpy().squeeze())\n",
    "    else:\n",
    "        for p in model.features:\n",
    "            if p.__class__.__name__==\"Linear\":\n",
    "                res+=np.linalg.norm(p.weight.detach().numpy().squeeze())\n",
    "\n",
    "    return res\n",
    "\n",
    "def two_relu_layer_train(args, num_neurons=100):\n",
    "    epochs = args['epochs']\n",
    "#     path = '/home/huijin/seminar project/model/'\n",
    "\n",
    "    #     tag='sgd_epoch'+str(epochs)+'_init_'+str(initialization)+'_lr_'+str(learning_rate)+'_bs_'+str(batch_size)+'_network_'+network_choice\n",
    "    tag = args['tag']+str(num_neurons)\n",
    "    input_dim = args['input_dim']\n",
    "    \n",
    "    model = TwoLayerReluASI(input_dim=input_dim, num_neurons=num_neurons, \n",
    "                            initialization=args['weight'], bias_tune_tuple=args['bias'])\n",
    "\n",
    "    global best_acc\n",
    "    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "    use_cuda = True\n",
    "    best_loss = 1000000\n",
    "    old_loss = 10\n",
    "    learning_rate = 0.1 #0.02\n",
    "    #     momentum = 0.9\n",
    "    #     weight_decay = 5e-4\n",
    "    # Data\n",
    "    #     dataloader = datasets.CIFAR10\n",
    "    #     train_batch = batch_size #128\n",
    "    #     test_batch = 100\n",
    "    workers = 1\n",
    "    decay_epoch = 100000\n",
    "    use_cuda = False\n",
    "\n",
    "    #     if accumulation_steps*batch_size/learning_rate>=4096/0.02:\n",
    "    #         decay_epoch = 250\n",
    "    #         epochs = 400\n",
    "    #     elif accumulation_steps*batch_size/learning_rate>=4096/0.04:\n",
    "    #         decay_epoch = 200\n",
    "    #         epochs = 350\n",
    "\n",
    "#     xp = np.array([-2, -1, 0, 1, 2],dtype=np.float)\n",
    "#     fp = [1.5, 0.5, 1.5, 0.5, 1.5]\n",
    "    xp = args['xp']\n",
    "    fp = args['fp']\n",
    "    # train_input = np.sort(np.concatenate((np.random.uniform(-2,2,30),xp))).astype(np.float32)\n",
    "#     train_input = np.sort(xp).astype(np.float32)\n",
    "#     train_output = np.interp(train_input, xp, fp).astype(np.float32)\n",
    "    train_input = xp\n",
    "    train_output = fp\n",
    "\n",
    "    trainset = [(train_input[i:i+1], train_output[i:i+1]) for i in range(train_input.shape[0])]\n",
    "    # batch_size=train_input.shape[0]\n",
    "    trainloader = data.DataLoader(trainset, batch_size=train_input.shape[0], shuffle=False, num_workers=workers)\n",
    "\n",
    "    #     testset = dataloader(root='/home/huijin/large-batch-training-torch/data', train=False, download=False, transform=transform_test)\n",
    "    #     testloader = data.DataLoader(testset, batch_size=test_batch, shuffle=False, num_workers=workers)\n",
    "\n",
    "    #     model = AlexNet(10)\n",
    "    #     model_state = torch.load('model_38')\n",
    "    #     model.load_state_dict(model_state)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    #     optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay= weight_decay)\n",
    "    if args['train_only_output']:\n",
    "        model.features1[0].weight.requires_grad = False\n",
    "        model.features1[0].bias.requires_grad = False\n",
    "        model.features2[0].weight.requires_grad = False\n",
    "        model.features2[0].bias.requires_grad = False\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda == True:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    # Initialization\n",
    "\n",
    "    #     initial_model_file = '/home/huijin/large-batch-training-torch/model/init_'+str(initialization)+'_network_'+network_choice\n",
    "    #     if os.path.isfile(initial_model_file):\n",
    "    #         model_state = torch.load(initial_model_file)\n",
    "    #         model.load_state_dict(model_state)\n",
    "    #     else:\n",
    "    #         torch.save(model.state_dict(), initial_model_file)\n",
    "    inital_norm_weight = sum_of_all_parameter(model)\n",
    "    # Train and val\n",
    "    history = dict(loss=list(),val_loss=list(),acc=list(),val_acc=list())\n",
    "    old_loss = test(trainloader, model, criterion, 0, use_cuda)+0.01\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "#         import ipdb; ipdb.set_trace() \n",
    "        old_state_dict = copy.deepcopy(model.state_dict())\n",
    "#         print(old_state_dict)\n",
    "#         import ipdb; ipdb.set_trace()\n",
    "        train_loss = train(trainloader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    #         test_loss, test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "        train_loss = test(trainloader, model, criterion, epoch, use_cuda)\n",
    "        print('Epoch: [%d | %d] LR: %f; Train Loss %f'\n",
    "              % (epoch + 1, epochs, optimizer.param_groups[0]['lr'],train_loss))\n",
    "        sys.stdout.flush()\n",
    "        history['loss'].append(train_loss)\n",
    "\n",
    "        # save model\n",
    "        is_best = train_loss < best_loss\n",
    "        best_loss = min(train_loss, best_loss)\n",
    "        if is_best:\n",
    "            torch.save(model.state_dict(), args['path']+tag+'_best'+\"normal [-1,1]\")\n",
    "        # torch.save(model.state_dict(), '/home/huijin/large-batch-training-torch/model/'+tag+'model_batch_'+str(epoch+1))\n",
    "\n",
    "        if train_loss > old_loss:\n",
    "            model.load_state_dict(old_state_dict)\n",
    "            learning_rate = learning_rate*0.5\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] *= 1.0/2\n",
    "        else:\n",
    "            if epoch % decay_epoch == (decay_epoch-1):\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] *= 0.5\n",
    "            if abs(old_loss-train_loss)<1e-8:\n",
    "                break\n",
    "            old_loss = train_loss\n",
    "\n",
    "    torch.save(model.state_dict(), args['path']+tag)\n",
    "    \n",
    "    end_norm_weight = sum_of_all_parameter(model)\n",
    "    \n",
    "#     test_input = np.arange(-2 , 2 , 0.01).astype(np.float32)\n",
    "#     testset = [(test_input[i:i+1]) for i in range(test_input.shape[0])]\n",
    "#     testloader = data.DataLoader(testset, batch_size=test_input.shape[0], shuffle=False, num_workers=workers)\n",
    "#     for batch_idx, inputs in enumerate(testloader):\n",
    "#         outputs = model(inputs)\n",
    "#     test_output = outputs.squeeze().detach().numpy()\n",
    "#     true_output = np.interp(test_input, xp, fp)\n",
    "#     away_from_piecewise_linear = np.mean((true_output-test_output)**2)\n",
    "    return (inital_norm_weight, end_norm_weight, 0, train_loss, model)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\n",
    "       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.square_sum = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.square_sum += val * val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.std = self.square_sum / self.count - self.avg*self.avg\n",
    "        \n",
    "        \n",
    "# args['nums_of_neurons']=[10,20,40,80,160,320,640,1280,2560]\n",
    "\n",
    "inital_norm_weight_vs_num = []\n",
    "inital_norm_weight_std_vs_num = []\n",
    "end_norm_weight_vs_num = []\n",
    "end_norm_weight_std_vs_num = []\n",
    "away_from_variational_vs_num = []\n",
    "away_from_variational_std_vs_num = []\n",
    "train_loss_vs_num = []\n",
    "train_loss_std_vs_num = []\n",
    "\n",
    "for num_of_neurons in args['nums_of_neurons']:\n",
    "    number_of_try = 3\n",
    "    inital_norm_weights = AverageMeter()\n",
    "    end_norm_weights = AverageMeter()\n",
    "    away_from_variationals = AverageMeter()\n",
    "    train_losses = AverageMeter()\n",
    "    for init in range(number_of_try):\n",
    "        inital_norm_weight, end_norm_weight, away_from_variational, train_loss, model = two_relu_layer_train(args, num_of_neurons)\n",
    "        test_input = np.concatenate((X, Y), axis=2)\n",
    "        test_input = test_input.reshape(-1,2)\n",
    "        testset = [(test_input[i:i+1]) for i in range(test_input.shape[0])]\n",
    "        testloader = data.DataLoader(testset, batch_size=numgrid*numgrid, shuffle=False, num_workers=1)\n",
    "        for batch_idx, inputs in enumerate(testloader):\n",
    "            outputs = model(inputs)\n",
    "        test_output = outputs.squeeze().detach().numpy()\n",
    "        test_output = test_output.reshape(numgrid, numgrid)\n",
    "        away_from_variational = np.sum((test_output - f_variation_exact)**2)*h*h\n",
    "        inital_norm_weights.update(inital_norm_weight)\n",
    "        end_norm_weights.update(end_norm_weight)\n",
    "        away_from_variationals.update(away_from_variational)\n",
    "        train_losses.update(train_loss)\n",
    "\n",
    "    inital_norm_weight_vs_num.append(inital_norm_weights.avg)\n",
    "    end_norm_weight_vs_num.append(end_norm_weights.avg)\n",
    "    away_from_variational_vs_num.append(away_from_variationals.avg)\n",
    "\n",
    "    inital_norm_weight_std_vs_num.append(inital_norm_weights.std)\n",
    "    end_norm_weight_std_vs_num.append(end_norm_weights.std)\n",
    "    away_from_variational_std_vs_num.append(away_from_variationals.std)\n",
    "\n",
    "    train_loss_vs_num.append(train_losses.avg)\n",
    "    train_loss_std_vs_num.append(train_losses.std)\n",
    "\n",
    "print(inital_norm_weight_vs_num)\n",
    "print(end_norm_weight_vs_num)\n",
    "print(away_from_variational_vs_num)\n",
    "print(train_loss_vs_num)\n",
    "\n",
    "print(inital_norm_weight_std_vs_num)\n",
    "print(end_norm_weight_std_vs_num)\n",
    "print(away_from_variational_std_vs_num)\n",
    "print(train_loss_std_vs_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting 3D surfaces of output of the network output and the solution of the variational problem\n",
    "fx = x.copy()\n",
    "fy = y.copy()\n",
    "fX, fY = np.meshgrid(fx, fy)\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# X, Y, Z = axes3d.get_test_data(0.05)\n",
    "# ax.plot_wireframe(fX[120:-120,120:-120], fY[120:-120,120:-120], f[120:-120,120:-120])\n",
    "ax.plot_wireframe(fX, fY, f_variation_exact)\n",
    "ax.plot_wireframe(fX, fY, test_output, color=\"red\")\n",
    "ax.scatter3D(args['xp'][:,0].squeeze(), args['xp'][:,1].squeeze(), args['fp'].squeeze(), c=\"red\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code of plotting contour of the solution of the variational problem\n",
    "fig, ax = plt.subplots()\n",
    "CS2 = ax.contour(fX, fY, f_variation_exact, levels=np.linspace(-0.4, 1.4, 10, dtype=np.float32))\n",
    "ax.clabel(CS2, inline=True, fontsize=15)\n",
    "ax.scatter(args['xp'][:,0].squeeze(), args['xp'][:,1].squeeze(), c=\"red\");\n",
    "plt.title(\"Exact Solution\", fontsize=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the difference between the network output and the solution of the variational problems by a power function\n",
    "import numpy as np\n",
    "x=args['nums_of_neurons']\n",
    "y=away_from_variational_vs_num\n",
    "error=away_from_variational_std_vs_num\n",
    "from sklearn.linear_model import LinearRegression\n",
    "start = 0\n",
    "n = np.log(x[start:]).shape[0]\n",
    "X = np.zeros((n,1))\n",
    "X[:,0] = np.log(x[start:])\n",
    "# X[:,1] = np.ones(n)\n",
    "y_s = np.log(np.sqrt(y[start:]))\n",
    "reg = LinearRegression().fit(X, y_s)\n",
    "reg.score(X, y_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the difference between the network output and the solution of the variational problem against the number of neurons\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax0= plt.subplots()\n",
    "# ax0.errorbar(x[1:], np.sqrt(y[1:]), yerr=np.sqrt(error[1:])/np.sqrt(y[1:])/2, fmt='-o')\n",
    "# ax0.loglog(x[start:], np.sqrt(y[start:]), '-o')\n",
    "plt.xlabel(\"Number of neurons\")\n",
    "plt.ylabel(\"Error\")\n",
    "n_line = np.linspace(10,5120,1000).shape[0]\n",
    "X_line = np.zeros((n_line,1))\n",
    "X_line[:,0] = np.linspace(10,5120,1000)\n",
    "ax0.set_xscale(\"log\", nonposx='clip')\n",
    "ax0.set_yscale(\"log\", nonposy='clip')\n",
    "# ax4.set(title='Errorbars go negative')\n",
    "ax0.errorbar(x[start:], np.sqrt(y[start:]), yerr=(np.sqrt(error)/np.sqrt(y)/2)[start:], fmt='-o')\n",
    "ax0.loglog(X_line[:,0], np.exp(reg.predict(np.log(X_line))))\n",
    "ax0.legend([f\"$y={np.exp(reg.intercept_):.4f}x^{{{reg.coef_[0]:.4f}}}$\",'Error'],fontsize='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following code reads the trained model from the file, so they can be run separately once you run the above code and have the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting 3D surfaces of output of the network output and the solution of the variational problem\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "args['xp'] = np.array([[-1,-1], [1,1], [0,0], [-1,1], [1,-1]],dtype=np.float32)\n",
    "args['fp'] = np.array([[1.5], [1.5], [0.5], [-0.5], [-0.5]],dtype=np.float32)\n",
    "args['input_dim'] = 2\n",
    "args['tag'] = \"2d_toy\"\n",
    "\n",
    "# number of neurons in hidden layers, you can change this value\n",
    "num_neurons = 20\n",
    "# num_neurons = 160\n",
    "\n",
    "model = TwoLayerReluASI(input_dim=2, num_neurons=num_neurons)\n",
    "args['path'] = '/Users/jinhui/Documents/GitHub/model'\n",
    "tag = args['tag']+str(num_neurons)\n",
    "\n",
    "model_state = torch.load(args['path']+tag+'_best'+\"normal [-1,1]\")\n",
    "model.load_state_dict(model_state)\n",
    "\n",
    "numgrid = 21\n",
    "R = np.abs(args['xp']).max()\n",
    "x = np.linspace(-R, R, numgrid, dtype=np.float32)\n",
    "h = x[1]-x[0]\n",
    "y = np.linspace(-R, R, numgrid, dtype=np.float32)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "X = np.expand_dims(X, axis=2)\n",
    "Y = np.expand_dims(Y, axis=2)\n",
    "test_input = np.concatenate((X, Y), axis=2)\n",
    "test_input = test_input.reshape(-1,2)\n",
    "\n",
    "testset = [(test_input[i:i+1]) for i in range(test_input.shape[0])]\n",
    "testloader = data.DataLoader(testset, batch_size=numgrid*numgrid, shuffle=False, num_workers=1)\n",
    "for batch_idx, inputs in enumerate(testloader):\n",
    "    outputs = model(inputs)\n",
    "test_output = outputs.squeeze().detach().numpy()\n",
    "test_output = test_output.reshape(numgrid, numgrid)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# X, Y, Z = axes3d.get_test_data(0.05)\n",
    "# ax.plot_wireframe(fX[120:-120,120:-120], fY[120:-120,120:-120], f[120:-120,120:-120])\n",
    "ax.plot_wireframe(fX, fY, f_variation_exact)\n",
    "ax.plot_wireframe(fX, fY, test_output, color=\"red\")\n",
    "ax.scatter3D(args['xp'][:,0].squeeze(), args['xp'][:,1].squeeze(), args['fp'].squeeze(), c=\"red\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code of plotting contour of the network output\n",
    "fig, ax = plt.subplots()\n",
    "# CS = ax.contour(fX, fY, f_variation_exact, 40)\n",
    "# ax.clabel(CS, inline=True, fontsize=10)\n",
    "CS2 = ax.contour(fX, fY, test_output, np.linspace(-0.4, 1.4, 10, dtype=np.float32))\n",
    "ax.clabel(CS2, inline=True, fontsize=15)\n",
    "ax.scatter(args['xp'][:,0].squeeze(), args['xp'][:,1].squeeze(), c=\"red\");\n",
    "plt.title(f\"n={num_neurons}\", fontsize=22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
